{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "python-notes",
      "language": "python",
      "name": "python-notes"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aicrashcoursewinter24/adria-csc-480/blob/Transformers-Lab/Transformer_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9xm6FgFEICV"
      },
      "source": [
        "# Transformer-based Language Model - GPT2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozq819nh8Tik"
      },
      "source": [
        "- This notebook runs on Google Colab.\n",
        "- Codes from [A Comprehensive Guide to Build Your Own Language Model in Python](https://medium.com/analytics-vidhya/a-comprehensive-guide-to-build-your-own-language-model-in-python-5141b3917d6d)\n",
        "- Use the OpenAI GPT-2 language model (based on Transformers) to:\n",
        "  - Generate text sequences based on seed texts\n",
        "  - Convert text sequences into numerical representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TE3aQiE82dq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efea7409-feb6-4ddd-c31b-7d1eb109a0a1"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrbLcMec8ibI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e45291f-193c-49d7-e4b7-cbbb959f532d"
      },
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Encode a text inputs\n",
        "text = \"What is the fastest car in the\"\n",
        "indexed_tokens = tokenizer.encode(text)\n",
        "\n",
        "# Convert indexed tokens in a PyTorch tensor\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Set the model in evaluation mode to deactivate the DropOut modules\n",
        "model.eval()\n",
        "\n",
        "# If you have a GPU, put everything on cuda\n",
        "tokens_tensor = tokens_tensor.to('cuda')\n",
        "model.to('cuda')\n",
        "\n",
        "# Predict all tokens\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "    predictions = outputs[0]\n",
        "\n",
        "# Get the predicted next sub-word\n",
        "predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
        "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
        "\n",
        "# Print the predicted word\n",
        "print(predicted_text)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the fastest car in the world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuxymOoC9JUj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "922ae8b3-93c4-4a15-8590-d4672e568887"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 178464, done.\u001b[K\n",
            "remote: Counting objects: 100% (591/591), done.\u001b[K\n",
            "remote: Compressing objects: 100% (268/268), done.\u001b[K\n",
            "remote: Total 178464 (delta 339), reused 485 (delta 281), pack-reused 177873\u001b[K\n",
            "Receiving objects: 100% (178464/178464), 198.53 MiB | 22.58 MiB/s, done.\n",
            "Resolving deltas: 100% (124725/124725), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8546wsN9bY1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b4ce6ec-a90e-4997-fe58-e11aa110f5ec"
      },
      "source": [
        "!ls transformers/examples"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flax  legacy  pytorch  README.md  research_projects  run_on_remote.py  tensorflow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tMDvf-d9b98"
      },
      "source": [
        "## Text Generation Using DPT2\n",
        "\n",
        "- [Write with Transformer](https://transformer.huggingface.co/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQmIXydd9QdP"
      },
      "source": [
        "# !python transformers/examples/text-generation/run_generation.py \\\n",
        "#     --model_type=gpt2 \\\n",
        "#     --model_name_or_path=gpt2 \\\n",
        "#     --length=100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlbLrtELB3nv"
      },
      "source": [
        "## Text Generation Using GPT2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYZrmw4p-eZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "797fe331-a6a7-4481-cd54-48c352d9d26e"
      },
      "source": [
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "generator(\"Hello, I'm a language model,\", max_length=50, num_return_sequences=5)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"Hello, I'm a language model, but what I'm really doing is making a human-readable document. There are other languages, but those are the ones I like the most. To do your research, please contact me, this isn't your\"},\n",
              " {'generated_text': \"Hello, I'm a language model, not a syntax model. That's why I like it. I've done a lot of programming projects.\\n\\nBut my job as a C programmer is to sort through every single line of the script so I\"},\n",
              " {'generated_text': \"Hello, I'm a language model, and I'll do it in no time!\\n\\nOne of the things we learned from talking to my friend from college a bit earlier, and in the context of the current language model I think it's important\"},\n",
              " {'generated_text': 'Hello, I\\'m a language model, not a command line tool.\\n\\nIf my code is simple enough:\\n\\nif (use (string-replace \"\\\\r\" ))) {\\n\\nconsole. log\\n\\n}\\n\\nthat\\'s'},\n",
              " {'generated_text': \"Hello, I'm a language model, I've been using Language in all my work. Just a small example, let's see a simplified example. I'm making an API for a game where I want a character to play a little bit of a\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9gp2M-C_vfQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ffbb5d8-dcf6-481f-bee8-8a536a0a9f9e"
      },
      "source": [
        "generator(\"Scrat is a fictional character in the Ice Age franchise. He is a saber-toothed squirrel who is obsessed with collecting acorns, constantly putting his life in danger to obtain and defend them. Scrat's storylines are mostly\", max_length=75, num_return_sequences=5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"Scrat is a fictional character in the Ice Age franchise. He is a saber-toothed squirrel who is obsessed with collecting acorns, constantly putting his life in danger to obtain and defend them. Scrat's storylines are mostly based on real life and fantasy shows like The Harry Potter movies. In the same way that the Wild West has been a staple of the\"},\n",
              " {'generated_text': \"Scrat is a fictional character in the Ice Age franchise. He is a saber-toothed squirrel who is obsessed with collecting acorns, constantly putting his life in danger to obtain and defend them. Scrat's storylines are mostly based on old stories, like those about the two former ice beasts.\\n\\nAscension:\\n\\nThe first Apocryphal\"},\n",
              " {'generated_text': \"Scrat is a fictional character in the Ice Age franchise. He is a saber-toothed squirrel who is obsessed with collecting acorns, constantly putting his life in danger to obtain and defend them. Scrat's storylines are mostly the result of the events of The Snow White and the Huntsman (1989) and The Snow White and the Huntsman (1994-\"},\n",
              " {'generated_text': \"Scrat is a fictional character in the Ice Age franchise. He is a saber-toothed squirrel who is obsessed with collecting acorns, constantly putting his life in danger to obtain and defend them. Scrat's storylines are mostly in regards to his life story, as well as his family, although his mother is also his family's chief target. As scat\"},\n",
              " {'generated_text': \"Scrat is a fictional character in the Ice Age franchise. He is a saber-toothed squirrel who is obsessed with collecting acorns, constantly putting his life in danger to obtain and defend them. Scrat's storylines are mostly based on actual historical events and historical situations from the Ice Age. There are also fictional depictions of Chewbacca in the movies as well\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOiRd7fhAgW7"
      },
      "source": [
        "## Transforming Texts into Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4dbcEWhAdRW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d12825dc-6806-459f-b862-7dc0d9b7985e"
      },
      "source": [
        "# from transformers import GPT2Tokenizer, GPT2Model\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "# model = GPT2Model.from_pretrained('gpt2')\n",
        "# text = \"Replace me by any text you'd like.\"\n",
        "# encoded_input = tokenizer(text, return_tensors='pt') # return tensorflow tensors\n",
        "# output = model(encoded_input)\n",
        "\n",
        "\n",
        "from transformers import GPT2Tokenizer, TFGPT2Model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = TFGPT2Model.from_pretrained('gpt2')\n",
        "text = \"Scrat is a fictional character in the Ice Age franchise. He is a saber-toothed squirrel who is obsessed with collecting acorns, constantly putting his life in danger to obtain and defend them. Scrat's storylines are mostly\"\n",
        "encoded_input = tokenizer(text, return_tensors='tf')\n",
        "output = model(encoded_input)\n",
        "print(encoded_input)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2Model.\n",
            "\n",
            "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(1, 48), dtype=int32, numpy=\n",
            "array([[ 3351, 10366,   318,   257, 19812,  2095,   287,   262,  6663,\n",
            "         7129,  8663,    13,   679,   318,   257, 17463,   263,    12,\n",
            "           83,  1025,   704, 33039,   508,   318, 21366,   351, 13157,\n",
            "          936, 19942,    11,  7558,  5137,   465,  1204,   287,  3514,\n",
            "          284,  7330,   290,  4404,   606,    13,  1446, 10366,   338,\n",
            "        44880,   389,  4632]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 48), dtype=int32, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1]], dtype=int32)>}\n"
          ]
        }
      ]
    }
  ]
}